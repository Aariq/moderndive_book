# (PART) Inference via infer {-} 

# Sampling {#sampling}

```{r setup_infer, include=FALSE, purl=FALSE}
chap <- 8
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

# This bit of code is a bug fix on asis blocks, which we use to show/not show LC
# solutions, which are written like markdown text. In theory, it shouldn't be
# necessary for knitr versions <=1.11.6, but I've found I still need to for
# everything to knit properly in asis blocks. More info here: 
# https://stackoverflow.com/questions/32944715/conditionally-display-block-of-markdown-text-using-knitr
library(knitr)
knit_engines$set(asis = function(options) {
  if (options$echo && options$eval) knit_child(text = options$code)
})

# This controls which LC solutions to show. Options for solutions_shown: "ALL"
# (to show all solutions), or subsets of c('5-1', '5-2','5-3', '5-4'), including
# the null vector c('') to show no solutions.
solutions_shown <- c('')
show_solutions <- function(section){
  return(solutions_shown == "ALL" | section %in% solutions_shown)
  }
```


In this chapter we kick off the third segment of this book, statistical inference, by learning about **sampling**. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we'll cover in Chapters \@ref(ci) and \@ref(hypo) respectively. We will see that the tools that you learned in the data science segment of this book (data visualization, "tidy" data format, and data wrangling) will also play an important role here in the development of your understanding.  As mentioned before, the concepts throughout this text all build into a culmination allowing you to "think with data."

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(moderndive)
# For loading CSV files:
library(readr)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(knitr)
library(patchwork)

set.seed(79)
```


## Sampling bowl

Let's start this chapter with a sampling exercise. Imagine you are given a large bowl with 2400 balls that are either red, white, or green. We are interested in the proportion of balls in this bowl that are red, but are too lazy to do an exhaustive count. You are also given a "shovel" that you can insert into this bowl...

```{r sampling-exercise-1, echo=FALSE, fig.cap="A bowl with 2400 balls", purl=FALSE, out.width = "600px"}
knitr::include_graphics("images/sampling_bowl_2.jpg")
```

... and extract a sample of $n=50$ balls:

```{r sampling-exercise-2, echo=FALSE, fig.cap="A shovel used to extract a sample of size n = 50", purl=FALSE, out.width = "600px"}
knitr::include_graphics("images/sampling_bowl_3_cropped.jpg")
```



### General sampling terminology

Before we proceed, let's define some terminology:

1. **Population**: The population is a set of $N$ observations we are interested in.
    + In our example, its the bowl of $N=2400$ balls. 
1. **Population parameter**: A population parameter is a numerical summary measure about the population.
    + In our example, it's the true *population proportion $p$* of the balls in the bowl that are red. 
1. **Census**: An exhaustive enumeration/counting of all observations in the population used to compute the population parameter *exactly*, much like  the Decennial United States census attempts to exhaustively count the US population.
    + In our example, this would correspond to exhaustively counting all red balls of the $N=2400$ total balls and computing $p$ exactly. 
    + When $N$ is small, a census is feasible. However, when $N$ is large census can get very expensive, either in terms of time, energy, or money. 
1. **Sampling**: Collecting a subset of size $n \leq N$ of observations from the population. Typically $n$ is much smaller than $N$. 
    + In our example, this corresponds to using the shovel to extract a sample of $n=50$ balls.
1. **Representative sampling**: A sample is said be a representative sample if it "looks like the population".
    + In other words, the characteristics of sample's observations are a good representation of the characteristics of the population's observations.
    + In our example, this means does our sample of $n=50$ balls "look like" the contents of the bowl?
1. **Point estimates/sample statistics**: A summary statistic based on the sample of size $n$ that *estimates* the population parameter.
    + In our example, it's the *sample proportion $\widehat{p}$* of the balls in the sample of $n=50$ balls that are red. 
    + Key: The sample proportion $\widehat{p}$ is an estimate of the population proportion $p$.
1. **Generalizability**: We say a sample is generalizable if any results of based on the sample can generalize to the population.
    + In our example, is $\widehat{p}$ a "good guess" of $p$?
    + In other words, can we *infer* about the population based on our sample? 
1. **Bias**: In a statistical sense, we say bias occurs if certain observations of a population have a higher chance of being sampled than others. We say a sampling procedure is *unbiased* if every observation in a population had an equal chance of being sample.d
    + In our example, did each ball, irrespective of color, have an equal chance of being sampled?
1. **Random sampling**: We say a sampling procedure is random if we sample randomly from the population in an unbiased fashion.

**The moral**:

* If the sampling of a sample of size $n$ is done at random then
* The sample is unbiased and representative of the population thus
* Any result based on the sample can generalize to the population thus
* The point estimate/sample statistic is a "good guess" of the population parameter

**So in our example**:

* If we *properly mix the balls i.e. stir the bowl* first before using the shovel to extract a sample of size $n=50$
* The contents of the shovel will "look like" the contents of the bowl thus
* Any results based on the $n=50$ balls in the shovel can generalize to the $N=2400$ balls in the bowl thus
* The sample proportion $\widehat{p}$ of the $n=50$ balls in the shovel that are red is a "good guess" of the true population proportion $p$ of the $N=2400$ balls that are red. 


## Tactile sampling from bowl {#tactile}

### Tactilely using shovel once

Let's now put our sampling shovel into action in the following *tactile* sampling exercise. Tactile just means physically tangible and perceptible by touch. 

1. Step 1 in Figure \@ref(fig:tactile1): Use the shovel to take a sample of size $n=50$ balls from the bowl
1. Step 2 in Figure \@ref(fig:tactile2): Pour them into a Red Solo Cup and
    + Count the number that are red then
    + Compute the sample proportion $\widehat{p}$ of the $n=50$ balls that are red.
1. Step 3 in Figure \@ref(fig:tactile3): Mark the sample proportion $\widehat{p}$ in a hand-drawn histogram.

```{r tactile1, echo=FALSE, fig.cap="Step 1: Take sample of size $n=50$", purl=FALSE, out.width = "600px"}
knitr::include_graphics("images/sampling/tactile_1_b.jpg")
```

```{r tactile2, echo=FALSE, fig.cap="Step 2: Pour into Red Solo Cup and compute $\\widehat{p}$", purl=FALSE, out.width = "400px"}
knitr::include_graphics("images/sampling/tactile_2_a.jpg")
```

```{r tactile3, echo=FALSE, fig.cap="Step 3: Mark $\\widehat{p}$'s in histogram", purl=FALSE, out.width = "600px"}
knitr::include_graphics("images/sampling/tactile_3_a.jpg")
```

So for example in Figure \@ref(fig:tactile1) there are 18 balls out of $n=50$ that are red. The sample proportion red for this particular sample is thus $\widehat{p} = \frac{16}{50} = 0.32$. Our intrepid students then marked this value in the hand-drawn histogram in Figure \@ref(fig:tactile3). After 10 groups of students completed this exercise, the resulting hand-drawn histogram is in Figure \@ref(fig:tactile4) below. Observe:

* Five of the sample proportions $\widehat{p}$ (based on five different samples of size $n=50$) were in the histogram bin $[0.30, 0.35)$. In other words, sample proportions of $\widehat{p} = 0.30$ would be included in this bin, but sample proportions of $\widehat{p} = 0.35$ would be included in the next bin to the right.
* The lowest value of $\widehat{p}$ was somewhere between 0.20 and 0.25
* The highest value of $\widehat{p}$ was somewhere between 0.45 and 0.50.

```{r tactile4, echo=FALSE, fig.cap="Step 3: Histogram of 10 values of $\\widehat{p}$", purl=FALSE, out.width = "600px"}
knitr::include_graphics("images/sampling/tactile_3_c.jpg")
```

### Tactilely using shovel 33 times

All told, 33 groups tooks samples: in other words the shovel was used 33 times and 33 values of the sample proportion $\widehat{p}$ were computed. Let's import this data for all 33 groups and save it in a data frame called `tactile_prop_red` in Table \@ref(tab:tactile-prop-red). Notice how the `replicate` column enumerates each of the 33 groups, `red_balls` is the count of balls in the sample of size $n=50$ that we red, and `prop_red` is the sample proportion $\widehat{p}$ that are red. 

```{r, eval=FALSE}
library(readr)
tactile_prop_red <- read_csv("https://rudeboybert.github.io/STAT135/static/sampling_red_balls.csv")
View(tactile_prop_red)
```
```{r tactile-prop-red, echo=FALSE, message=FALSE, warning=FALSE}
tactile_prop_red <- read_csv("https://rudeboybert.github.io/STAT135/static/sampling_red_balls.csv")
tactile_prop_red %>% 
  kable(
    digits = 2,
    caption = "33 sample proportions based on 33 tactile samples of size n=50", 
    booktabs = TRUE
  )
```

### Sampling distribution

Using your data visualization skills that you honed in Chapter \@ref(viz), let's visualize the distribution of these 33 sample proportions red $\widehat{p}$ using a histogram. This visualization is appropriate since `prop_red` is a numerical variable. 

```{r samplingdistribution-tactile, echo=FALSE, fig.cap="Histogram of 33 sample proportions based on 33 tactile samples of size n=50"}
tactile_histogram <- ggplot(tactile_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, color = "white")
tactile_histogram + 
    labs(x = "Sample proportion red based on n = 50", title = "Histogram of 33 sample proportions based on 33 tactile samples of size n=50") 
```

Let's ask ourselves some questions:

1. Where is the histogram centered? 
1. What is the spread of this histogram?

Recall from Section \@ref(summarize) the mean and the standard deviation are two summary statistics that would answer this question:

```{r, eval=FALSE}
tactile_prop_red %>% 
  summarize(mean = mean(prop_red), sd = sd(prop_red))
```
```{r, echo=FALSE}
summary_stats <- tactile_prop_red %>% 
  summarize(mean = mean(prop_red), sd = sd(prop_red))
summary_stats %>% 
  kable(digits = 3)
```

What you have just unpacked are some very deep and very subtle concepts in statistical inference:

1. The histogram in Figure \@ref(fig:samplingdistribution-tactile) is called the **sampling distribution** of $\widehat{p}$ based on samples of size $n=50$. It describes how values of the sample proportion red will vary from sample to sample due to **sampling variability**. It allows us to identify:
    + Typical/common/plausible values of $\widehat{p}$. Ex: $\widehat{p} = 0.36$ would be such a value since it would in theory occur frequently.
    + Atypical/rare/implausible values of $\widehat{p}$. Ex: $\widehat{p} = 0.8$ would be such a value since it lies far away from most of the distribution.
1. If the sampling is done in an unbiased and random fashion, in other words we made sure to stir the bowl before we sampled, then the sampling distribution will be guaranteed to be centered at the true unknown population proportion red $p$, or in other words the true number of balls out of 2400 that are red.
1. The spread of this histogram, as quantified by the standard deviation of `r summary_stats %>% pull(sd) %>% round(3)`, is called the **standard error**. It quantifies the variability of our estimates for $\widehat{p}$.
    + **Note**: A large source of confusion. All standard errors are a form of standard deviation, but not all standard deviations are standard errors.



## Virtual sampling from bowl

Now let's mimic the above *tactile* sampling, but with *virtual* sampling. In other words:

* Instead of considering the *tactile bowl* shown in Figure \@ref(fig:sampling-exercise-1) above and using a *tactile shovel* to draw samples of size $n=50$
* Let's use a *virtual bowl* saved in a computer and use R's random number generator as a *virtual shovel* to draw samples of size $n=50$

First, we describe our *virtual bowl*. In the `moderndive` package, we've included a data frame called `bowl` that has 2400 rows corresponding to the $N=2400$ balls in the physical bowl. Run `View(bowl)` in RStudio to convince yourselves that `bowl` is indeed a virtual version of the tactile bowl in the previous section.

```{r}
bowl
```

Note that the balls are not actually marked with numbers; the variable `ball_ID` is merely used as an identification variable for each row of `bowl`. Recall our previous discussion on identification variables in Subsection \@ref(identification-vs-measurement) in the "Data Tidying" Chapter \@ref(tidy). 

Next, we describe our *virtual shovel*: the `rep_sample_n()` function included in the `moderndive` package where `rep_sample_n()` indicates that we are taking repeated/replicated samples of size $n$.

### Virtually using shovel once

The `rep_sample_n()` function included in the `moderndive` package where `rep_sample_n()` indicates that we are taking repeated/replicated samples of size $n$. Let's perform the virtual analogue of tactilely inserting the shovel *only once* into the bowl and extracting a sample of `size` $n=50$. In the table below we only show results about the first 10 sampled balls out of 50.


```{r, eval=FALSE}
virtual_shovel <- bowl %>% 
  rep_sample_n(size = 50)
View(virtual_shovel)
```
```{r, echo=FALSE}
virtual_shovel <- bowl %>% 
  rep_sample_n(size = 50)
virtual_shovel %>% 
  slice(1:10) %>%
  knitr::kable(
    align = c("r", "r"),
    digits = 3,
    caption = "First 10 sampled balls of 50 in virtual sample",
    booktabs = TRUE
  )
```

Looking at all 50 rows of `virtual_shovel` in the spreadsheet viewer that pops up after running `View(virtual_shovel)`, the `ball_ID` variable seems to suggest that we do indeed have a random sample of $n=50$ balls. However what does the `replicate` variable indidate, where in this case it's equal to 1 for all 50 rows? We'll see in a minute. First let's compute the both the number of balls red and the proportion red out of $n=50$ using our `dplyr` data wrangling tools from Chapter \@ref(wrangling):

```{r, eval=FALSE}
virtual_shovel %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)
```
```{r, echo=FALSE}
virtual_shovel %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50) %>% 
  knitr::kable(
    digits = 3,
    caption = "Count and proportion red in single virtual sample of size n = 50",
    booktabs = TRUE
  )
```

Why does this work? Because for every row where `color == "red"`, the boolean `TRUE` is returned and R treats `TRUE` like the number `1`. Equivalently, for every row where `color` is not equal to `"red"`, the boolean `FALSE` is returned and R treats `FALSE` like the number `0`. So summing the number of `TRUE`'s and `FALSE`'s is equivalent to summing `1`'s and `0`'s which counts the number of balls where `color` is `red`. 



### Virtually using shovel 33 times

Recall however in our tactile sampling exercise in Section \@ref(tactile) above that we had 33 groups of students take 33 samples of size $n=50$ using the shovel 33 times and hence compute 33 separate values of the sample proportion red $\widehat{p}$. In other words we *repeated/replicated* the sampling 33 times. We can achieve this by reusing the same `rep_sample_n()` function code above, but by adding the `reps = 33` argument indicating we want to repeat this sampling 33 times:

```{r, eval=FALSE}
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 33)
View(virtual_samples)
```
```{r, echo=FALSE}
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 33)
```

`virtual_samples` has $50 \times 33 = 1650$ rows, corresponding to 33 samples of size $n=50$, or 33 draws from the shovel. We won't display the contents of this data frame but leave it to you to `View()` this data frame. You'll see that the first 50 rows have `replicate` equal to 1, then the next 50 rows have `replicate` equal to 2, and so on and so forth, up until the last 50 rows which have `replicate` equal to 33. The `replicate` variable denotes which of our 33 samples a particular ball is included in.

Now let's compute the 33 correponsding values of the sample proportion $\widehat{p}$ based on 33 different samples of size $n=50$ by reusing the previous code, but remembering to `group_by` the `replicate` variable first since we want to compute the sample proportion. Notice the similarity of this table with Table \@ref(tab:tactile-prop-red).

```{r, eval=FALSE}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)
View(virtual_prop_red)
```
```{r virtual-prop-red, echo=FALSE}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)
virtual_prop_red %>% 
  kable(
    digits = 2,
    caption = "33 sample proportions red based on 33 virtual samples of size n=50", 
    booktabs = TRUE
  )
```

Just as we did before, let's draw a histogram of the 33 sample proportions $\widehat{p}$

```{r samplingdistribution-virtual, echo=FALSE, fig.cap="Histogram of 33 sample proportions red based on 33 virtual samples of size n=50"}
virtual_histogram <- ggplot(virtual_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, color = "white")
virtual_histogram +
  labs(x = "Sample proportion red based on n = 50", title = "Histogram of 33 sample proportions based on 33 virtual samples of size n=50") 
```

Let's now compare the:

* Tactile sampling-based sampling distribution from the previous section to the
* Virtual sampling-based sampling distribution from this section

```{r, echo=FALSE}
tactile_histogram <- tactile_histogram +
  labs(x = "Sample proportion red based on n = 50", title = "Tactile sampling distribution")
virtual_histogram <- virtual_histogram +
  labs(x = "Sample proportion red based on n = 50", title = "Virtual sampling distribution")
tactile_histogram + virtual_histogram
```

We see that they are similar and center and spread, although not identical due to random variation.


### Virtually using shovel 1000 times

In Figure \@ref(fig:samplingdistribution-virtual), we can start seeing a pattern in the sampling distribution emerge. However, 33 values of the sample proportion $\widehat{p}$ might not be enough to get a true sense of the distribution. Using 1000 values of $\widehat{p}$ would definitely give a better sense. What are our two options for constructing the these histograms?

1. Tactile sampling: Make the 33 groups of students take $\frac{1000}{33} \approx 31$ samples of size $n=50$ each, count the number of red balls 31 times each, and compute 31 values of the sample proportion $\widehat{p}$. This would be cruel and unusual as this would take hours!
1. Virtual sampling: Computer are very good at automating repetitive and boring tasks such as this one. This is the way to go!

First, generate 1000 samples of size n=50

```{r, eval=FALSE}
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 1000)
View(virtual_samples)
```
```{r, echo=FALSE}
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 1000)
```

Then for each of these 1000 samples of size n=50, compute the corresponding sample proportions

```{r, eval=FALSE}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)
View(virtual_prop_red)
````
```{r, echo=FALSE}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)
```

Finally plot it

```{r, eval=FALSE}
ggplot(virtual_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, color = "white") +
  labs(x = "Sample proportion red based on n = 50", title = "Histogram of 1000 sample proportions based on 1000 virtual samples of size n=50") 
```
```{r, echo=FALSE}
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red/50)

ggplot(virtual_prop_red, aes(x = prop_red)) +
  geom_histogram(binwidth = 0.05, color = "white") +
  labs(x = "Sample proportion red based on n = 50", title = "Histogram of 1000 sample proportions based on 1000 virtual samples of size n=50") 
```


```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is the standard error of the above sampling distribution of $\widehat{p}$ based on 1000 samples of size $n=50$?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Change the sample size to $n=25$? What difference do you notice about the sampling distribution and the standard error?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Change the sample size to $n=100$? What difference do you notice about the sampling distribution and the standard error?



```{block, type='learncheck', purl=FALSE}
```



## Central Limit Theorem

What you have just shown in the previous section is a very famous theorem, or mathematically proven truth, called the *Central Limit Theorem*. It loosely states that when samples means and sample proportions are based on larger and larger samples, the sampling distribution corresponding to these point estimates get

1. More and more normal
1. More and more narrow

Shuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created the following 3m38s video explaining this crucial theorem to statistics using as examples, what else?

1. The average weight of wild bunny rabbits!
1. The average wing span of dragons!

<center>
<iframe width="800" height="450" src="https://www.youtube.com/embed/jvoxEYmQHNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>




## Conclusion

### What's to come?

This chapter serves as an introduction to the theoretical underpinning of the statistical inference techniques that will be discussed in greater detail in Chapter \@ref(ci) for confidence intervals and Chapter \@ref(hypo) for hypothesis testing. 

### Script of R code

An R script file of all R code used in this chapter is available [here](https://moderndive.netlify.com/scripts/08-sampling.R).

